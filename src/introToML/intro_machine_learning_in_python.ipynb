{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n",
    "\n",
    "In this tutorial, you will learn how to build a machine learning model in Python. The focus of the turorial is as follows: how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. We will use the `scikit-learn` package in Python to accomplish all machine learning tasks. We will use simple datasets for practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "    - [Glossary of Terms](#Glossary-of-Terms)\n",
    "- [Python Setup](#Python-Setup)\n",
    "- [The Machine Learning Process](#The-Machine-Learning-Process)\n",
    "- [Problem Formulation](#Problem-Formulation)\n",
    "    - [Four Main Types of ML Tasks for Policy Problems](#Four-Main-Types-of-ML-Tasks-for-Policy-Problems)\n",
    "    - [Our Machine Leaning Problem](#Our-Machine-Learning-Problem)\n",
    "- [Data Exploration and Preparation](#Data-Exploration-and-Preparation)\n",
    "- [Building a Model and Model Fitting](#Building-a-Model-and-Model-Fitting)\n",
    "    - [Training and Test Sets](#Training-and-Test-Sets)\n",
    "    - [Class Balancing](#Class-Balancing)\n",
    "    - [Crosstabs](#Crosstabs)\n",
    "    - [Splitting into Features and Labels](#Splitting-into-Features-and-Labels)\n",
    "- [Model Understanding and Evaluation](#Model-Understanding-and-Evaluation)\n",
    "    - [Running a Machine Learning Model](#Running-a-Machine-Learning-Model)\n",
    "    - [Model Understanding](#Model-Understanding)\n",
    "    - [Model Evaluation](#Model-Evaluation)\n",
    "    - [Confusion Matrix](#Confusion-Matrix)\n",
    "    - [Precision and Recall at k%](#Precision-and-Recall-at-k%)\n",
    "- [Machine Learning Pipeline](#Machine-Learning-Pipeline)\n",
    "- [Survey of Algorithms](#Survey-of-Algorithms)\n",
    "- [Assessing Model Against Baselines](#Assessing-Model-Against-Baselines)\n",
    "- [Exercise](#Exercise)\n",
    "- [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary of Terms\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "There are a number of terms specific to Machine Learning that you will find being mentioned repeatedly in this notebook. Please refer to this Glossary to remind yourself of their meanings.\n",
    "\n",
    "- **Learning**: In Machine Learning, you'll hear about \"learning a model.\" This is what you probably know as \n",
    "*fitting* or *estimating* a function in Statistics, Mathematics or Econometrics. Other terms include *training* or *building* a model. These terms are all synonyms and are \n",
    "used interchangeably in the machine learning literature.\n",
    "- **Training Examples**: These are what you probably know as *data points* or *observations* or *rows*. They are called training examples because we know their labels.\n",
    "- **Features**: These are what you probably know as *independent variables*, *attributes*, *predictors*, \n",
    "or *explanatory variables.*\n",
    "- **Instances**: There are basically our *observations* or in some disciplines *cases*. In a typical ML setup, one row equals one instance.\n",
    "- **Underfitting**: This happens when a model is too simple and does not capture the structure of the data well \n",
    "enough.\n",
    "- **Overfitting**: This happens when a model is too complex or too sensitive to the noise in the data; this can\n",
    "result in poor generalization performance, or applicability of the model to new data. \n",
    "- **Regularization**: This is a general method to avoid overfitting by applying additional constraints to the model. \n",
    "For example, you can limit the number of features present in the final model, or the weight coefficients applied\n",
    "to the (standardized) features are small.\n",
    "- **Supervised learning** A type of learning in which we have training exampleas with labels for each example. **The labels refer to one target or outcome variable (continuous or discrete) that we want\n",
    "to predict, or classify data into**. Classification, prediction, and regression fall into this category. We call the\n",
    "set of explanatory variables $X$ **features**, and the outcome variable of interest $Y$ the **label**.\n",
    "- **Unsupervised learning** involves problems that do not have a specific outcome variable of interest, but rather\n",
    "we are looking to understand \"natural\" patterns or groupings in the data - looking to uncover some structure that \n",
    "we do not know about a priori. Clustering is the most common example of unsupervised learning, another example is \n",
    "principal components analysis (PCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this tutorial. We're already familiar with `numpy` and `pandas` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to build machine learnign models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "# from __future__ import division\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\":1.25, \"lines.markersize\":8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The general process to process to use Machine Learning to solve problems can ge describe as follows:\n",
    "\n",
    "- [**Problem description.**](#problem-formulation) \n",
    "The goal here is to understand the problem you are trying to solve. During this stage, you can also determine if this problem is amenable to be solved using Machine Learning. For instance, the goal could be to improve health outcomes or understand the effect of a \n",
    "variable *X* on an outcome *Y*, etc. At this stage, it is important to work with people who understand the domain being\n",
    "studied to dig deeper and define the problem more concretely. What is the analytical formulation of the metric \n",
    "that you are trying to optimize?\n",
    "- [**Formulate it as a machine learning problem.**](#problem-formulation) Is it a classification problem or a regression problem? Is the \n",
    "goal to build a model that generates a ranked list prioritized by risk, or is it to detect anomalies as new data \n",
    "come in? Knowing what kinds of tasks machine learning can solve will allow you to map the problem you are working on\n",
    "to one or more machine learning settings and give you access to a suite of methods.\n",
    "- [**Data preparation.**](#Data Preparation) Next, you need to carefully explore the data you have. Do you even have enough enough data or relevant data? In some cases, you may need to collect additional data. Is your data labelled or not-do you have training data? For instance, you may be interested in detecting fradaulent transactions in health insurance claims, but the machine learning method you will use and the type of output you get from the model  will depend on whether you have labelled data or not. Its also crucial to explore the data to understand structure of the data such as distribution of key variables, whether you have missing values in the data etc.\n",
    "- [**Feature engineering.**](#feature-generation) In machine learning language, features refer to what is called variables in Statistics or Econometrics or predictors when you use model fitting language.\n",
    "Creating good features is probably the most important step in the \n",
    "machine learning process. They are no hard and fast rules for this process but it may involve any of the following steps: manually picking which variables to use based on domain knowledge, transforming some variables, creating new variables baased on interactions of two or more variables or aggregating variables \n",
    "over time and space.\n",
    "- **Model selection.** A model here refers to a method or technique or algorithm which you choose to use. Since they are so many off-the-shelf algorithms available, careful model selection is an important step in the process. Typically, in machine learning, you take a set of models and try all of them, empirically validate which one is giving you the best perfomance based on your metrics.\n",
    "- [**Evaluation.**](#evaluation) As you build a large number of possible models, you need a way select the best one among them. In the model evaluation stage, you determine which is the best based on the data you have. \n",
    "- [**Deployment.**](#deployment) Once you have selected the best model and validated it using historical data you are ready to put the model into production. In some case, this step may  ot be necessary. FOr instance, if the project is a one off project, then you may only need to run the model a few number of times and write a report or analysis about it. However, in most cases, the model is put into production which means its running continuosly on historical and new data. Think of Amazon predicting whether you will buy a product or not.\n",
    "\n",
    "\n",
    "If you have studied Statistics or Economics, you will notice that the process of model building in both disciplines exploit statistical properties in the data. Thus, model fitting in Statistics or economics or other social sciences is  is similar to machine learning. However, one key difference is that in Statistics or Economics, the focus is often *intepretation*. Thus, you usually start with a hypothesis about the underlying process that gave rise to your data, chose an appropriate model based on prior knowledge and fit it using least squares, and used the resulting parameter or coefficient estimates (or confidence intervals) for inference. This type of modeling is very useful for *interpretation*. In machine learning, the primary concern is *prediction*. rather than interpretation. In this regard, we make a few notes about the machine learning process:\n",
    "\n",
    "- **The goal is to find a model which generalises well to the data we haven't seen** This means that we won't evaluate our model's performance using the same data (training data) that we used to fit the model on but rather a hold out dataset which the model didnt see during the fitting or training process.\n",
    "- **We care less about the structure of the model and more about the performance** This means that we'll try out a whole bunch of models at a time and choose the one that works best, rather than determining which model to use ahead of time. However, you can still select a *suboptimal* model if you care about a specific model type or  model interpretability\n",
    "- **We can include a lot of variables in the model.** This may sound like the complete opposite of what you've heard in the past, and it can be hard to swallow. But we will use different methods to deal with many of those concerns in the model fitting process by using a more automatic variable selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Iris Dataset\n",
    "The data we will use in our tutorial is the Iris dataset, its one of the classical dataset in machine learning and statistics. In the dataset, each instance is a species of the flower Iris with corresponding atributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../../day4-intro-machine-learning/other-resources/iris.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This is an important first step which involves turning your goal or objective into a form which can be understood in machine learning terms. For instance, the overall project goal may be to *reduce girls drop out rate* in elementary school. You may have data about characteristics of female students. In terms of machine learning, the problem can be formulated in terms of *predicting female students who will drop out*.\n",
    "\n",
    "### Two of the Main ML TasksFour Main Types of ML Tasks for Policy Problems\n",
    "\n",
    "- **Classification**:Classification tasks is simply related with predicting a category of a data (discrete variables). One of the most common example is predicting whether or not an email if spam or ham. Some of the common use cases could be found in the area of healthcare such as whether a person is suffering from a particular disease or not\n",
    "- **Regression**: Regression tasks mainly deal with estimation of numerical values (continuous variables). Some of the examples include estimation of housing price, product price, stock price etc.\n",
    "\n",
    "### Our Machine Learning Problem for Tutorial\n",
    "For this tutorial, for the sake of learning, we are looking at a dataset of flowers of type Iris. In the dataset, each flower has four numeric attributes and a class which describes the species of flower. They are three species. Our task is to *classify* each instance of a flower into a class using the attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In our case, we are using a toy dataset to illustrate the process of building models and mostof the steps mentioned below may not apply. However, we still mention them to emphasize their importance. In general, data prepration will involve the following steps:\n",
    "\n",
    "1. **Generating training data**: Since our problem is a supervised learning one, we need data with labels. Labels are the dependent variables, or *Y* variables, that we are trying to predict. In case where you dont have labels, you may need to do extra work to generate labels and this is almost always a manual process. The labels can be either *binary* or *multiclass*. \n",
    "> In our dataset, the outcome variable is not *binary* since we have 3 classes. \n",
    "\n",
    "1. **Decide on feature**: Features are our independent variables or predictors. Good features make machine learning systems effective. The better the features the easier it is the capture the structure of the data. One way to generate features is using domain knowledge. However, modern algorithms are trying more and more to automatically generate features. \n",
    "> For the problem we are solving in this tutorial, there isn't any feature selection because we arleady have very few features but in paractice you will find that you will spend alot of time trying to decide what features to use in your model.\n",
    "\n",
    "1. **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text. Example of feature engineering are: \n",
    "    - **Transformations**, such as converting a variable into  log, square, and square root.\n",
    "    - **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables (such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "    - **Discretization**. Several methods require features to be discrete instead of continuous.\n",
    "    - **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several values into one feature, aggregating over varying windows of time and space. For example, given urban data, we would want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    "1. **Cleaning data**: This can involve several steps whose purpose is to make sure that the data makes sense. Some things which can be done include the following: \n",
    "    - **Missing values.** Missing values can be imputed or in some cases the instances with missing values can be dropped from the model building process. Its important to check that treating missing values doesnt alter the underlying distribution in the data.\n",
    "    - **Outliers.** Some wildly outling values can distort the distribution of the data and also affect perfomance of models or even predictability of the target variable. Therefore, outliers need to be handled by either excluding them from the model building process or replacing them with sensible values.\n",
    "\n",
    "1. **Scaling features**: Machine learning models use optimisation algorithms under the hood to find the best parameters. For some optimisation techniques variables with values which vary wildly do affect the convergenc process. In this regard, it may be necessary to *scale* some features. One way of scaling features with a continuous scale is to normalise them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "*In practice you can combine the previous step with this one*.\n",
    "Before building a machine learning model it is necessary to inspect the data. While you may have arleady \n",
    "inspected the data during the preparation stage its often good to do further exploration before you start building building the model. They are many reasons why this is crucially important:\n",
    " - to see if the task is really solvable with machine learning\n",
    " - to check if you missed any abnormalities and oddities in the data \n",
    " \n",
    "One of the best ways to inspect data is to visualize it. Lets explore the dataset we will use in the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check basic characteristics of the data\n",
    "As explained earlier, its crucial to understand the data before you start building ML models. We can check the following things about the data:\n",
    "- How many instances\n",
    "- How many attributes/features/variables\n",
    "- How many classes for the target variable\n",
    "- Distribution of instances across the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the data using pandas\n",
    "iris_file = os.path.abspath('../../day4-intro-machine-learning/data/iris.csv')\n",
    "df = pd.read_csv(iris_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets view the data\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many instances?\n",
    "print('==========================')\n",
    "print(' Number of Instances')\n",
    "print('==========================')\n",
    "print('They are {} instances in the dataset '.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How attributes\n",
    "print() # to create space\n",
    "print('==========================')\n",
    "print(' Number of Attributes')\n",
    "print('==========================')\n",
    "columns = df.columns\n",
    "# remove class_label to remain with attributes\n",
    "features = list(set(columns)-set(['species']))\n",
    "print('Attributes: {}'.format(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution across classes\n",
    "print() # to create space\n",
    "print('=========================================')\n",
    "print(' Instances Distribution Across Classes')\n",
    "print('=========================================')\n",
    "display(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "g = sns.pairplot(df, hue='species', markers='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.violinplot(y='species', x='sepal_length', data=df, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='species', x='sepal_width', data=df, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='species', x='petal_length', data=df, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='species', x='petal_width', data=df, inner='quartile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As we mentioned earlier on, its not enough to just build the model; we're going to need a way to know whether or not it generalizes to new data. In other words, we need metrics to determine whether our model is perfoming to satisfication. This is crucial not only for us as people developing the model but also for others to trust results and decisions from the model. This is where **model evaluation** comes in. \n",
    "\n",
    "To convince ourselves - and others - that our modeling results will generalize, we need to hold some data back (not using it to train/fit the model), then apply our model to that hold-out dataset and \"blindly\" predict, comparing the model's predictions to what we actually observed. This is called **cross-validation**, and it's the best way we have to estimate how a model will perform on *entirely* novel data. We call the data used to build the model the **training set**, and the rest the **test set**.\n",
    "\n",
    "In general, we'd like our training set to be as large as possible, to allow our model to be built with as much data as possible. However, you also want to be as confident as possible that your model will generalize to new data. In practice, you'll have to balance these two objectives in a reasonable way.  \n",
    "\n",
    "There are also many ways to split up your data into training and testing sets. Since you're trying to evaluate how your model will perform *in practice*, it's best to emulate the true use case of your model as closely as possible when you decide how to evaluate it. A good [tutorial on cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) can be found on the `scikit-learn` site.\n",
    "\n",
    "They are several ways to split data into training and test set. One simple but not very robust approach is to set 70% of the data for training and 30% for testing. A more robust approach is ***k-fold* cross-validation**, which entails splitting up our dataset into *k* groups, holding out one group while training a model on the rest of the data, evaluating model performance on the held-out \"fold,\" and repeating this process *k* times. In essense, the former is a special case of the latter. In this tutorial, for the sake of simplicity, we will not do k-fold cross validation but just split the data into train and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Fitting a Model\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We need to transform our dataset into **features** (predictors, or independent variables, or $X$ variables) and **labels** (dependent variables, or $Y$ variables).  For ease of reference, in subsequent examples, names of variables that pertain to predictors will start with \"`X_`\", and names of variables that pertain to outcome variables will start with \"`y_`\".  Every machine learning package has its own requirements for accepting data for building a model and our task is to ensure that our data is in a form thats acceptable for the framework we are using. We will soon what this means for scikit-learn. For instance, we saw that our target variable has categorical labels, most machine learning models dont accept these type of label. Therefore, we will need to convert the target variable into a numeric variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Sets\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, for building models using scikit-learn, we need to split the features (*X*) and target variables (*Y*) into separate numpy arrays. Once we have done that, we will split both the *X* array and the *Y* array into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Predictors/Features and what we are predicting (Labels)\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We arleady checked how many attributes we have in our dataset. Since we have very few attributes, we use all of them as features.  Our target variable in this case is the species of the flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features by removing the species column\n",
    "features = list(set(columns)-set(['species']))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df[features]\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five columns of features array:\\n{}\".format(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable into a Y array\n",
    "Y = df['species'].values\n",
    "print(\"First five columns of labels array:\\n{}\".format(Y[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn function to split into train and test test\n",
    "# This can also be achieved easily without \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "# Lets checkout how the splits look\n",
    "print()\n",
    "print(\"========= Train and Test Sets Splits ==========\")\n",
    "print(\"X_train shape: {}\".format(X_train.shape)) \n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Understanding and Evaluation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can finally fit a model on our training set. The training set's features will be used to predict the labels. Once our model is created using the training set, we will assess its quality by applying it to the test set: by comparing the *predicted values* to the *actual values* for each record in the testing data set. \n",
    "\n",
    "- **Performance Evaluation**: How well will our model do once it is deployed and applied to new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Machine Learning Model\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We picked [`scikit-learn`](http://scikit-learn.org/stable/) because it is a commonly used and well documented Python library for machine learning. In addition, it is very easy to use compared to other libraries. The library has alot of functionalities including helping you to split your data into training and test sets as we saw, fit models and use them to predict results on new data, and evaluate your results.\n",
    "\n",
    "We will start with the simplest [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model and see how well that does.\n",
    "\n",
    "You can use any number of metrics to judge your models (see [model evaluation](#model-evaluation)), but we'll use [`accuracy_score()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) (ratio of correct predictions to total number of predictions) as our measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/define the model object\n",
    "model = LogisticRegression(penalty='l1', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "model.fit(X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model results, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `intercept_scaling`, `max_iters`, `penalty`, and `solver`).  Example output:\n",
    "\n",
    "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0)\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `max_iter` of 1000, `intercept_scaling` of 2, and `solver` of \"lbfgs\" (pulled from thin air as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression( max_iter = 1000, intercept_scaling = 2, solver = \"lbfgs\" )\n",
    "\n",
    "The basic way to choose values for, or \"tune,\" these parameters is the same as the way you choose a model: fit the model to your training data with a variety of parameters, and see which perform the best on the test set. An obvious drawback is that you can also *overfit* to your test set; in this case, you can alter your method of cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check how scikit-learn has arranged the classes\n",
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Understanding\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "After running our logistic regression, let's look at the coefficients for each feature, along with their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the zip function maps elements from containers in the same position\n",
    "print (\"The coefficients for each of the features are ....\")\n",
    "\n",
    "# we need to use set to print the values because zip returns an iterator object\n",
    "set(zip(features, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly or in this case the class for that instance. Rather, models produce a score between 0 and 1 (that can sometimes be interpreted as a probability), which lets you more finely rank all of the examples from *most likely* to *least likely* to belong to a certain class (e.g., 1 (positive) for binary classification). Since for most use cases, we may need the actual class label to be predicted, in order to turn this score into a class label or in case of binary classification 0 or 1, a user specifies a threshold. For example, in a binary classfication, you might label all examples that have a score greater than 0.5 as positive (1), while in a multiclass classification as in the example we have now, the instance is given a class with highest score. Its important to note that the user can play around with these scores depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In order to get the scores, we run the model on our test data-pretty \n",
    "# much the process of multipying the coefficients with the feature values\n",
    "y_scores = model.predict_proba(X_test)\n",
    "y_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lets remind ourselves how the classes are ordered in the model object: \\n{}'.format(model.classes_))\n",
    "print('\\n And we can see that the highest score for the first instance is for class \"Iris-virginica\" and \\n thats whats assigned by scikit-learn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We can look at the scores as percentages ....')\n",
    "for i in y_scores[:1]:\n",
    "    print()\n",
    "    print(set(zip(model.classes_, i*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once we have the predictions (a predicted class) for each instance, we can create a *confusion matrix*, which  has 9 cells: true negatives, true positives, false negatives, and false positives for each class. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"actual/predicted\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "y_actual = y_test # Note that these are the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted = y_predicted # what the model predicted\n",
    "actual = y_test # Note that these are the true labels\n",
    "conf_matrix = confusion_matrix(y_predicted,y_test, labels=model.classes_)\n",
    "print(\"            ======= CONFUSION MATRIX =======\")\n",
    "print()\n",
    "print_cm(conf_matrix, model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_actual, y_predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we think about this accuracy? good? bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional metrics that are often useful are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. For multi-class classification like in our case, precision is not always easy to interepret. We need to calculate precision for each class or use average precision for all classes. The formula for precision below applies for a binary classification. The same applies for recall.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. For binary classification, a conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_actual, y_predicted, average='macro')\n",
    "recall = recall_score(y_actual, y_predicted,average='macro')\n",
    "print( \"Average Precision = \" + str( precision ) )\n",
    "print( \"Average Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics.\n",
    "Depending on the type of problem you are solving, they are more metrics which you can use. Also, you can do fine grained evaluation:\n",
    "- [Area under the curve (AUC-PR)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic): If we care about our whole precision-recall space, we can optimize for a metric known as the area under the curve (AUC-PR), which is the area under the precision-recall curve. The maximum AUC-PR is 1.\n",
    "- [Precision and Recall at k](https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54): In recommendation systems, for instance, when Amazon recommends products to users, they would want to know out of the recommnded items, how many the user actually bought. Thus, they may focus on precision for the top 5 or 10 products instead of the global precision.\n",
    "- **Custom defined metrics**: Of course you can also define your own metrics if out of the box metrics dont work for your problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work be reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or the agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Algorithms\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We have only scratched the surface of what we can do with our model. We've only tried one classifier (Logistic Regression), and there are plenty more classification algorithms in `sklearn`. Let's try them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random classifier\n",
    "random_classifier = [random.choice([random.choice(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']) for i in enumerate(y_test)] ) for i in enumerate(y_test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=1000, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=1000, n_jobs=-1),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17\n",
    "                                         , n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(max_depth=10, min_samples_split=10),\n",
    "        'Random-classifier': random_classifier\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, clf in clfs.items(): \n",
    "    print()\n",
    "    print(\"=========================================\")\n",
    "    print(\"  Working on Model {} with Metrics Below \".format(model_name))\n",
    "    print(\"=========================================\")\n",
    "    \n",
    "    if model_name == 'Random-classifier':\n",
    "        predicted = clf\n",
    "    else:\n",
    "        clf.fit(X_train, y_train )\n",
    "        predicted = clf.predict(X_test)\n",
    "    \n",
    "    actual = y_test\n",
    "    precision = precision_score(actual, predicted, average='macro')\n",
    "    recall = recall_score(actual, predicted,average='macro')\n",
    "    print()\n",
    "    print( \"Average Precision => {:.2f}%\".format(precision*100) )\n",
    "    print( \"Average Recall => {:.2f}%\".format(recall*100))\n",
    "    \n",
    "\n",
    "    \n",
    "    if model_name == 'Random-classifier':\n",
    "        continue\n",
    "    \n",
    "    #feature importances\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        feature_import = dict(\n",
    "            zip(features,clf.coef_.ravel()))\n",
    "    elif hasattr(clf, 'feature_importances_'):\n",
    "        feature_import = dict(\n",
    "            zip(features, clf.feature_importances_))\n",
    "    print(\"FEATURE IMPORTANCES\")\n",
    "    print(feature_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing a Model Against Baselines\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. The common baselines include:\n",
    "- **Majority classifier:** As the name suggests, this is a model which always predicts the class which is in majority. For instance, given a hypothetical binary outcome variable where 80% of the instances are 1 and the rest are 0. This kind of classifier would have an ccuracy of 80%. Thus, when you have trained more sophisticated models, its important that they perfom better (the accuracy should be higher) than this.\n",
    "- **Random classifier:** This is a classifier assigns to each instance a label or class completely at random. For instance, in the binary classification case described above, this classifier should give us accuracy around 50%. For a multiclass problem like ours, a random classifier perfoms very badly. You can check how a random classifier does in comparison with the sophisticated models in the results on **survey of models** above.\n",
    "- ** Expert based baselines**: Based on your application, you could come up with other baselines based on expert knowledge other than the baselines above. \n",
    "\n",
    "> Its a good practice to compare your model with these baselines earlier on in the process before invest too much time in the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Predictions on the Titanic Dataset\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As a practice you will train a set of models on the titanic dataset and *select the one with best perfomance*. The reason I picked this dataset is because its small, clean and easy to use for quick practice. In real life, you will need to go the whole nine yards while here we will skip though some necessary steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The info below was copied from [Kaggle](https://www.kaggle.com/c/titanic) where I got the dataset.\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "** Variable description for selected variables**:\n",
    "- pclass: ticket class\n",
    "- sibsp: number of siblings / spouses aboard the Titanic\n",
    "- parch: number  of parents / children aboard the Titanic.\n",
    "- ticket: ticket number\n",
    "- fare: fare \n",
    "- cabin: cabin number\n",
    "- embarked: port of embarkation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../other-resources/titanic-sinks.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction Task\n",
    "The task is to predict sorts of people were likely to survive. As you will see, in the dataset, every instance represents a passenger and the target variable *Survived* represents whether that passenger survived the tragedy (1) or died (0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric\n",
    "For simplicity, we will use *accuracy* as a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Explore Data\n",
    "This will be very brief preparation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you use full path to the file\n",
    "#titanic_train = pd.read_csv('../data/titanic-train.csv')\n",
    "\n",
    "# Read the data into a dataframe as we have done before: replace None with your code\n",
    "df = pd.read_csv('../../day4-intro-machine-learning/data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the data using the info function\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decide on prediction Features\n",
    "By simple understanding of the problem, which features do you think wont be useful for prediction?\n",
    "- PassengerId, Name, Ticket: We drop these columns\n",
    "- Cabin: we drop this column because it has many missing values\n",
    "The structure of the train and test datasets has to be exactly the same, so we will do the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle Missing Features with Missing Values\n",
    "Notice that variables *Age*, *Embarked* and *Fare* have missing values in either training and/or test set. \n",
    "In scikit-learn, you cant buid a model on data with missing values, therefore, we have to care of them. We have two main choices: drop rows with missing values or impute them. We are going to impute the missing values using simple approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert Categorical Data to Numeric\n",
    "Sciki-learn doesnt accept string or categorical variables for features, therefore, we need to convert all categorical variables to numeric. We will take a simple approach here, just code the categories with integers. They are other approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Lets Build Some Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_train[features]\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Sex'] = df_train['Sex'].map( {'male': 1, 'female': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the data for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_classifier_titanic = [random.choice([0,1]) for i in enumerate(y_test)]\n",
    "\n",
    "# create majority classifier\n",
    "# majority_classifier = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a suite of models to test\n",
    "models_to_test = {'RF': RandomForestClassifier(n_estimators=1000, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=1000, n_jobs=-1),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17\n",
    "                                         , n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(max_depth=10, min_samples_split=10),\n",
    "        'Random-classifier': random_classifier_titanic\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, clf in models_to_test.items(): \n",
    "    print()\n",
    "    print(\"=========================================\")\n",
    "    print(\"  Working on Model {} with Metrics Below \".format(model_name))\n",
    "    print(\"=========================================\")\n",
    "    \n",
    "    if model_name == 'Random-classifier':\n",
    "        # this baseline model doesnt need fitting\n",
    "        predicted = clf\n",
    "    else:\n",
    "        # Fit the model\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predicted values from the model\n",
    "        predicted = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluating the model\n",
    "    actual = y_test\n",
    "    precision = precision_score(actual, predicted)\n",
    "    recall = recall_score(actual, predicted)\n",
    "    print()\n",
    "    print( \"  Precision => {:.2f}%\".format(precision*100) )\n",
    "    print( \"  Recall => {:.2f}%\".format(recall*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Model is the winner based on Precision?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Exercises for those who finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Try combination of different features and see how it changes the \n",
    "perfomance of the models\n",
    "2. Implement a **majority classifier** based on description above \n",
    "and compare its perfomance with the rest of the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "501px",
    "width": "485px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "554px",
    "left": "0px",
    "right": "1492px",
    "top": "111px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
